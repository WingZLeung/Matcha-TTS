[2025-01-17 17:36:53,670][matcha.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-01-17 17:36:53,756][matcha.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
CONFIG
├── data
│   └── _target_: matcha.data.text_mel_datamodule.TextMelDataModule             
│       name: torgo                                                             
│       train_filelist_path: /mnt/parscratch/users/acr22wl/TTDS_out/filelists/sp
│       valid_filelist_path: /mnt/parscratch/users/acr22wl/TTDS_out/filelists/sp
│       batch_size: 32                                                          
│       num_workers: 2                                                          
│       pin_memory: true                                                        
│       cleaners:                                                               
│       - english_cleaners2                                                     
│       add_blank: true                                                         
│       n_spks: 8                                                               
│       n_fft: 1024                                                             
│       n_feats: 80                                                             
│       sample_rate: 16000                                                      
│       hop_length: 256                                                         
│       win_length: 1024                                                        
│       f_min: 0                                                                
│       f_max: 8000                                                             
│       data_statistics:                                                        
│         mel_mean: -5.264439582824707                                          
│         mel_std: 1.8346129655838013                                           
│       seed: 1234                                                              
│       load_durations: false                                                   
│                                                                               
├── model
│   └── _target_: matcha.models.matcha_tts.MatchaTTS                            
│       n_vocab: 178                                                            
│       n_spks: 8                                                               
│       spk_emb_dim: 64                                                         
│       n_feats: 80                                                             
│       data_statistics:                                                        
│         mel_mean: -5.264439582824707                                          
│         mel_std: 1.8346129655838013                                           
│       out_size: null                                                          
│       prior_loss: true                                                        
│       use_precomputed_durations: false                                        
│       encoder:                                                                
│         encoder_type: RoPE Encoder                                            
│         encoder_params:                                                       
│           n_feats: 80                                                         
│           n_channels: 192                                                     
│           filter_channels: 768                                                
│           filter_channels_dp: 256                                             
│           n_heads: 2                                                          
│           n_layers: 6                                                         
│           kernel_size: 3                                                      
│           p_dropout: 0.1                                                      
│           spk_emb_dim: 64                                                     
│           n_spks: 1                                                           
│           prenet: true                                                        
│         duration_predictor_params:                                            
│           filter_channels_dp: 256                                             
│           kernel_size: 3                                                      
│           p_dropout: 0.1                                                      
│       decoder:                                                                
│         channels:                                                             
│         - 256                                                                 
│         - 256                                                                 
│         dropout: 0.05                                                         
│         attention_head_dim: 64                                                
│         n_blocks: 1                                                           
│         num_mid_blocks: 2                                                     
│         num_heads: 2                                                          
│         act_fn: snakebeta                                                     
│       cfm:                                                                    
│         name: CFM                                                             
│         solver: euler                                                         
│         sigma_min: 0.0001                                                     
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0001                                                            
│         weight_decay: 0.0                                                     
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
│         dirpath: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech/
│         filename: checkpoint_{epoch:03d}                                      
│         monitor: epoch                                                        
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 10                                                        
│         mode: max                                                             
│         auto_insert_metric_name: true                                         
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: 100                                                   
│         save_on_train_epoch_end: null                                         
│       model_summary:                                                          
│         _target_: lightning.pytorch.callbacks.RichModelSummary                
│         max_depth: 3                                                          
│       rich_progress_bar:                                                      
│         _target_: lightning.pytorch.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── tensorboard:                                                            
│         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
│         save_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech
│         name: null                                                            
│         log_graph: false                                                      
│         default_hp_metric: true                                               
│         prefix: ''                                                            
│                                                                               
├── trainer
│   └── _target_: lightning.pytorch.trainer.Trainer                             
│       default_root_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/lj
│       max_epochs: 100                                                         
│       accelerator: gpu                                                        
│       devices:                                                                
│       - 0                                                                     
│       precision: 16-mixed                                                     
│       check_val_every_n_epoch: 1                                              
│       deterministic: false                                                    
│       gradient_clip_val: 5.0                                                  
│                                                                               
├── paths
│   └── root_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS                      
│       data_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/data/                
│       log_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/                 
│       output_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech
│       work_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS                      
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── run_name
│   └── ljspeech                                                                
├── tags
│   └── ['ljspeech']                                                            
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── ckpt_path
│   └── None                                                                    
└── seed
    └── 1234                                                                    
[rank: 0] Seed set to 1234
[2025-01-17 17:36:54,341][__main__][INFO] - Instantiating datamodule <matcha.data.text_mel_datamodule.TextMelDataModule>
[2025-01-17 17:37:14,138][__main__][INFO] - Instantiating model <matcha.models.matcha_tts.MatchaTTS>
/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.
  deprecate("LoRACompatibleLinear", "1.0.0", deprecation_message)
[2025-01-17 17:37:23,460][__main__][INFO] - Instantiating callbacks...
[2025-01-17 17:37:23,460][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-01-17 17:37:23,561][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-01-17 17:37:23,562][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-01-17 17:37:23,562][__main__][INFO] - Instantiating loggers...
[2025-01-17 17:37:23,562][matcha.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>
[2025-01-17 17:37:23,565][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
Using 16bit Automatic Mixed Precision (AMP)
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[2025-01-17 17:37:23,654][__main__][INFO] - Logging hyperparameters!
[2025-01-17 17:37:27,724][__main__][INFO] - Starting training!
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃    ┃ Name                              ┃ Type              ┃ Params ┃ Mode  ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0  │ spk_emb                           │ Embedding         │    512 │ train │
│ 1  │ encoder                           │ TextEncoder       │  9.7 M │ train │
│ 2  │ encoder.emb                       │ Embedding         │ 34.2 K │ train │
│ 3  │ encoder.prenet                    │ ConvReluNorm      │  591 K │ train │
│ 4  │ encoder.prenet.conv_layers        │ ModuleList        │  553 K │ train │
│ 5  │ encoder.prenet.norm_layers        │ ModuleList        │  1.2 K │ train │
│ 6  │ encoder.prenet.relu_drop          │ Sequential        │      0 │ train │
│ 7  │ encoder.prenet.proj               │ Conv1d            │ 37.1 K │ train │
│ 8  │ encoder.encoder                   │ Encoder           │  8.7 M │ train │
│ 9  │ encoder.encoder.drop              │ Dropout           │      0 │ train │
│ 10 │ encoder.encoder.attn_layers       │ ModuleList        │  1.6 M │ train │
│ 11 │ encoder.encoder.norm_layers_1     │ ModuleList        │  3.1 K │ train │
│ 12 │ encoder.encoder.ffn_layers        │ ModuleList        │  7.1 M │ train │
│ 13 │ encoder.encoder.norm_layers_2     │ ModuleList        │  3.1 K │ train │
│ 14 │ encoder.proj_m                    │ Conv1d            │ 20.6 K │ train │
│ 15 │ encoder.proj_w                    │ DurationPredictor │  395 K │ train │
│ 16 │ encoder.proj_w.drop               │ Dropout           │      0 │ train │
│ 17 │ encoder.proj_w.conv_1             │ Conv1d            │  196 K │ train │
│ 18 │ encoder.proj_w.norm_1             │ LayerNorm         │    512 │ train │
│ 19 │ encoder.proj_w.conv_2             │ Conv1d            │  196 K │ train │
│ 20 │ encoder.proj_w.norm_2             │ LayerNorm         │    512 │ train │
│ 21 │ encoder.proj_w.proj               │ Conv1d            │    257 │ train │
│ 22 │ decoder                           │ CFM               │ 11.1 M │ train │
│ 23 │ decoder.estimator                 │ Decoder           │ 11.1 M │ train │
│ 24 │ decoder.estimator.time_embeddings │ SinusoidalPosEmb  │      0 │ train │
│ 25 │ decoder.estimator.time_mlp        │ TimestepEmbedding │  1.3 M │ train │
│ 26 │ decoder.estimator.down_blocks     │ ModuleList        │  3.1 M │ train │
│ 27 │ decoder.estimator.mid_blocks      │ ModuleList        │  2.8 M │ train │
│ 28 │ decoder.estimator.up_blocks       │ ModuleList        │  3.7 M │ train │
│ 29 │ decoder.estimator.final_block     │ Block1D           │  197 K │ train │
│ 30 │ decoder.estimator.final_proj      │ Conv1d            │ 20.6 K │ train │
└────┴───────────────────────────────────┴───────────────────┴────────┴───────┘
Trainable params: 20.9 M                                                        
Non-trainable params: 0                                                         
Total params: 20.9 M                                                            
Total estimated model params size (MB): 83                                      
Modules in train mode: 334                                                      
Modules in eval mode: 0                                                         
SLURM auto-requeueing enabled. Setting signal handlers.
/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
`Trainer.fit` stopped: `max_epochs=100` reached.
Epoch 99/99 ━━━━━━━━━━━━━━━━━ 58/58 0:00:16 • 0:00:00 3.72it/s v_num: 0.000     
                                                               step: 5799.000   
                                                               loss/train_step: 
                                                               3.902            
                                                               loss/val_step:   
                                                               4.709            
                                                               loss/val_epoch:  
                                                               5.335            
                                                               loss/train_epoch:
                                                               5.145            
[2025-01-17 18:05:29,207][__main__][INFO] - Starting testing!

[2025-01-17 18:05:29,234][matcha.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 94, in train
    trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 749, in test
    return call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 789, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 932, in _run
    _verify_loop_configurations(self)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 41, in _verify_loop_configurations
    __verify_eval_loop_configuration(model, "test")
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 106, in __verify_eval_loop_configuration
    raise MisconfigurationException(f"No `{step_name}()` method defined to run `Trainer.{trainer_method}`.")
lightning.fabric.utilities.exceptions.MisconfigurationException: No `test_step()` method defined to run `Trainer.test`.
[2025-01-17 18:05:29,282][matcha.utils.utils][INFO] - Output dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech/runs/2025-01-17_17-25-50
Error executing job with overrides: ['experiment=ljspeech']
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 117, in main
    metric_dict, _ = train(cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 87, in wrap
    raise ex
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 94, in train
    trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 749, in test
    return call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 789, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 932, in _run
    _verify_loop_configurations(self)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 41, in _verify_loop_configurations
    __verify_eval_loop_configuration(model, "test")
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 106, in __verify_eval_loop_configuration
    raise MisconfigurationException(f"No `{step_name}()` method defined to run `Trainer.{trainer_method}`.")
lightning.fabric.utilities.exceptions.MisconfigurationException: No `test_step()` method defined to run `Trainer.test`.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
`Trainer.fit` stopped: `max_epochs=100` reached.
Epoch 99/99 ━━━━━━━━━━━━━━━━━ 58/58 0:00:14 • 0:00:00 4.04it/s v_num: 0.000     
                                                               step: 5799.000   
                                                               loss/train_step: 
                                                               3.915            
                                                               loss/val_step:   
                                                               4.705            
                                                               loss/val_epoch:  
                                                               5.355            
                                                               loss/train_epoch:
                                                               5.153            
[2025-01-17 18:15:23,193][__main__][INFO] - Starting testing!

[2025-01-17 18:15:23,202][matcha.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 94, in train
    trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 749, in test
    return call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 789, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 932, in _run
    _verify_loop_configurations(self)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 41, in _verify_loop_configurations
    __verify_eval_loop_configuration(model, "test")
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 106, in __verify_eval_loop_configuration
    raise MisconfigurationException(f"No `{step_name}()` method defined to run `Trainer.{trainer_method}`.")
lightning.fabric.utilities.exceptions.MisconfigurationException: No `test_step()` method defined to run `Trainer.test`.
[2025-01-17 18:15:23,214][matcha.utils.utils][INFO] - Output dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech/runs/2025-01-17_17-36-53
Error executing job with overrides: ['experiment=ljspeech']
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 117, in main
    metric_dict, _ = train(cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 87, in wrap
    raise ex
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 94, in train
    trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 749, in test
    return call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 789, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 932, in _run
    _verify_loop_configurations(self)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 41, in _verify_loop_configurations
    __verify_eval_loop_configuration(model, "test")
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 106, in __verify_eval_loop_configuration
    raise MisconfigurationException(f"No `{step_name}()` method defined to run `Trainer.{trainer_method}`.")
lightning.fabric.utilities.exceptions.MisconfigurationException: No `test_step()` method defined to run `Trainer.test`.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
