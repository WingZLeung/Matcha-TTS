[2025-01-17 16:52:23,266][matcha.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-01-17 16:52:23,293][matcha.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
CONFIG
├── data
│   └── _target_: matcha.data.text_mel_datamodule.TextMelDataModule             
│       name: torgo                                                             
│       train_filelist_path: /mnt/parscratch/users/acr22wl/TTDS_out/filelists/sp
│       valid_filelist_path: /mnt/parscratch/users/acr22wl/TTDS_out/filelists/sp
│       batch_size: 32                                                          
│       num_workers: 2                                                          
│       pin_memory: true                                                        
│       cleaners:                                                               
│       - english_cleaners2                                                     
│       add_blank: true                                                         
│       n_spks: 8                                                               
│       n_fft: 1024                                                             
│       n_feats: 80                                                             
│       sample_rate: 16000                                                      
│       hop_length: 256                                                         
│       win_length: 1024                                                        
│       f_min: 0                                                                
│       f_max: 8000                                                             
│       data_statistics:                                                        
│         mel_mean: -5.264439582824707                                          
│         mel_std: 1.8346129655838013                                           
│       seed: 1234                                                              
│       load_durations: false                                                   
│                                                                               
├── model
│   └── _target_: matcha.models.matcha_tts.MatchaTTS                            
│       n_vocab: 178                                                            
│       n_spks: 8                                                               
│       spk_emb_dim: 64                                                         
│       n_feats: 80                                                             
│       data_statistics:                                                        
│         mel_mean: -5.264439582824707                                          
│         mel_std: 1.8346129655838013                                           
│       out_size: null                                                          
│       prior_loss: true                                                        
│       use_precomputed_durations: false                                        
│       encoder:                                                                
│         encoder_type: RoPE Encoder                                            
│         encoder_params:                                                       
│           n_feats: 80                                                         
│           n_channels: 192                                                     
│           filter_channels: 768                                                
│           filter_channels_dp: 256                                             
│           n_heads: 2                                                          
│           n_layers: 6                                                         
│           kernel_size: 3                                                      
│           p_dropout: 0.1                                                      
│           spk_emb_dim: 64                                                     
│           n_spks: 1                                                           
│           prenet: true                                                        
│         duration_predictor_params:                                            
│           filter_channels_dp: 256                                             
│           kernel_size: 3                                                      
│           p_dropout: 0.1                                                      
│       decoder:                                                                
│         channels:                                                             
│         - 256                                                                 
│         - 256                                                                 
│         dropout: 0.05                                                         
│         attention_head_dim: 64                                                
│         n_blocks: 1                                                           
│         num_mid_blocks: 2                                                     
│         num_heads: 2                                                          
│         act_fn: snakebeta                                                     
│       cfm:                                                                    
│         name: CFM                                                             
│         solver: euler                                                         
│         sigma_min: 0.0001                                                     
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0001                                                            
│         weight_decay: 0.0                                                     
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
│         dirpath: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech/
│         filename: checkpoint_{epoch:03d}                                      
│         monitor: epoch                                                        
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 10                                                        
│         mode: max                                                             
│         auto_insert_metric_name: true                                         
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: 100                                                   
│         save_on_train_epoch_end: null                                         
│       model_summary:                                                          
│         _target_: lightning.pytorch.callbacks.RichModelSummary                
│         max_depth: 3                                                          
│       rich_progress_bar:                                                      
│         _target_: lightning.pytorch.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── tensorboard:                                                            
│         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
│         save_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech
│         name: null                                                            
│         log_graph: false                                                      
│         default_hp_metric: true                                               
│         prefix: ''                                                            
│                                                                               
├── trainer
│   └── _target_: lightning.pytorch.trainer.Trainer                             
│       default_root_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/lj
│       max_epochs: -1                                                          
│       accelerator: gpu                                                        
│       devices:                                                                
│       - 0                                                                     
│       precision: 16-mixed                                                     
│       check_val_every_n_epoch: 1                                              
│       deterministic: false                                                    
│       gradient_clip_val: 5.0                                                  
│                                                                               
├── paths
│   └── root_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS                      
│       data_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/data/                
│       log_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/                 
│       output_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech
│       work_dir: /mnt/parscratch/users/acr22wl/Matcha-TTS                      
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── run_name
│   └── ljspeech                                                                
├── tags
│   └── ['ljspeech']                                                            
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── ckpt_path
│   └── None                                                                    
└── seed
    └── 1234                                                                    
[rank: 0] Seed set to 1234
[2025-01-17 16:52:23,627][__main__][INFO] - Instantiating datamodule <matcha.data.text_mel_datamodule.TextMelDataModule>
[2025-01-17 16:52:39,946][__main__][INFO] - Instantiating model <matcha.models.matcha_tts.MatchaTTS>
/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.
  deprecate("LoRACompatibleLinear", "1.0.0", deprecation_message)
[2025-01-17 16:52:54,362][__main__][INFO] - Instantiating callbacks...
[2025-01-17 16:52:54,363][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-01-17 16:52:54,476][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-01-17 16:52:54,476][matcha.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-01-17 16:52:54,476][__main__][INFO] - Instantiating loggers...
[2025-01-17 16:52:54,476][matcha.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>
[2025-01-17 16:52:54,479][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
Using 16bit Automatic Mixed Precision (AMP)
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[2025-01-17 16:52:54,563][__main__][INFO] - Logging hyperparameters!
[2025-01-17 16:53:00,458][__main__][INFO] - Starting training!
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃    ┃ Name                              ┃ Type              ┃ Params ┃ Mode  ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0  │ spk_emb                           │ Embedding         │    512 │ train │
│ 1  │ encoder                           │ TextEncoder       │  9.7 M │ train │
│ 2  │ encoder.emb                       │ Embedding         │ 34.2 K │ train │
│ 3  │ encoder.prenet                    │ ConvReluNorm      │  591 K │ train │
│ 4  │ encoder.prenet.conv_layers        │ ModuleList        │  553 K │ train │
│ 5  │ encoder.prenet.norm_layers        │ ModuleList        │  1.2 K │ train │
│ 6  │ encoder.prenet.relu_drop          │ Sequential        │      0 │ train │
│ 7  │ encoder.prenet.proj               │ Conv1d            │ 37.1 K │ train │
│ 8  │ encoder.encoder                   │ Encoder           │  8.7 M │ train │
│ 9  │ encoder.encoder.drop              │ Dropout           │      0 │ train │
│ 10 │ encoder.encoder.attn_layers       │ ModuleList        │  1.6 M │ train │
│ 11 │ encoder.encoder.norm_layers_1     │ ModuleList        │  3.1 K │ train │
│ 12 │ encoder.encoder.ffn_layers        │ ModuleList        │  7.1 M │ train │
│ 13 │ encoder.encoder.norm_layers_2     │ ModuleList        │  3.1 K │ train │
│ 14 │ encoder.proj_m                    │ Conv1d            │ 20.6 K │ train │
│ 15 │ encoder.proj_w                    │ DurationPredictor │  395 K │ train │
│ 16 │ encoder.proj_w.drop               │ Dropout           │      0 │ train │
│ 17 │ encoder.proj_w.conv_1             │ Conv1d            │  196 K │ train │
│ 18 │ encoder.proj_w.norm_1             │ LayerNorm         │    512 │ train │
│ 19 │ encoder.proj_w.conv_2             │ Conv1d            │  196 K │ train │
│ 20 │ encoder.proj_w.norm_2             │ LayerNorm         │    512 │ train │
│ 21 │ encoder.proj_w.proj               │ Conv1d            │    257 │ train │
│ 22 │ decoder                           │ CFM               │ 11.1 M │ train │
│ 23 │ decoder.estimator                 │ Decoder           │ 11.1 M │ train │
│ 24 │ decoder.estimator.time_embeddings │ SinusoidalPosEmb  │      0 │ train │
│ 25 │ decoder.estimator.time_mlp        │ TimestepEmbedding │  1.3 M │ train │
│ 26 │ decoder.estimator.down_blocks     │ ModuleList        │  3.1 M │ train │
│ 27 │ decoder.estimator.mid_blocks      │ ModuleList        │  2.8 M │ train │
│ 28 │ decoder.estimator.up_blocks       │ ModuleList        │  3.7 M │ train │
│ 29 │ decoder.estimator.final_block     │ Block1D           │  197 K │ train │
│ 30 │ decoder.estimator.final_proj      │ Conv1d            │ 20.6 K │ train │
└────┴───────────────────────────────────┴───────────────────┴────────┴───────┘
Trainable params: 20.9 M                                                        
Non-trainable params: 0                                                         
Total params: 20.9 M                                                            
Total estimated model params size (MB): 83                                      
Modules in train mode: 334                                                      
Modules in eval mode: 0                                                         
SLURM auto-requeueing enabled. Setting signal handlers.
/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
Epoch 0/-2                    0/58 0:00:01 • -:--:-- 0.00it/s v_num: 0.000      
                                                              loss/val_step:    
                                                              11.389            
                                                              loss/val_epoch:   
                                                              11.719            
[2025-01-17 16:53:17,346][matcha.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 84, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
    call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
    results = self._run_stage()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
    self.fit_loop.run()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 171, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py", line 79, in optimizer_step
    closure_result = closure()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 323, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/models/baselightningmodule.py", line 80, in training_step
    batch_size = batch[0].size(0) if isinstance(batch, (tuple, list)) else batch.size(0)
AttributeError: 'dict' object has no attribute 'size'
[2025-01-17 16:53:17,546][matcha.utils.utils][INFO] - Output dir: /mnt/parscratch/users/acr22wl/Matcha-TTS/logs/train/ljspeech/runs/2025-01-17_16-52-23
Error executing job with overrides: ['experiment=ljspeech']
Traceback (most recent call last):
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 117, in main
    metric_dict, _ = train(cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 87, in wrap
    raise ex
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/utils/utils.py", line 77, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/train.py", line 84, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
    call._call_and_handle_interrupt(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
    results = self._run_stage()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
    self.fit_loop.run()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 171, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py", line 79, in optimizer_step
    closure_result = closure()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 323, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/.conda/envs/match/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/mnt/parscratch/users/acr22wl/Matcha-TTS/matcha/models/baselightningmodule.py", line 80, in training_step
    batch_size = batch[0].size(0) if isinstance(batch, (tuple, list)) else batch.size(0)
AttributeError: 'dict' object has no attribute 'size'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
